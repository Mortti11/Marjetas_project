{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c9c6753",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'environment_conditions' from 'functions' (d:\\Fiveth\\Marjetas_project\\functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mW\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     environment_conditions,\n\u001b[0;32m     19\u001b[0m     detect_events,\n\u001b[0;32m     20\u001b[0m     compute_dry_streak,\n\u001b[0;32m     21\u001b[0m     compute_drying_times,\n\u001b[0;32m     22\u001b[0m     event_composite_environment,\n\u001b[0;32m     23\u001b[0m     visualize_one_selection,\n\u001b[0;32m     24\u001b[0m     create_event_viewer,\n\u001b[0;32m     25\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'environment_conditions' from 'functions' (d:\\Fiveth\\Marjetas_project\\functions.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from dash import Dash, dcc, html, Input, Output, dash_table\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "import plotly.express as px, plotly.graph_objects as go\n",
    "import ipywidgets as W\n",
    "from IPython.display import display\n",
    "\n",
    "from functions import (\n",
    "    environment_conditions,\n",
    "    detect_events,\n",
    "    compute_dry_streak,\n",
    "    compute_drying_times,\n",
    "    event_composite_environment,\n",
    "    visualize_one_selection,\n",
    "    create_event_viewer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ccbc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_progress_bars()\n",
    "hf_logging.disable_progress_bar()\n",
    "\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e8d121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "device_map = \"auto\" if torch.cuda.is_available() else None\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=dtype, device_map=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67dd9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, torch_dtype=dtype, device_map=device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecfa9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"You are an assistant that explains weather graphs. \" \"In simple words, explain what relative humidity means.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbd7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model output ===\n",
      "\n",
      "You are an assistant that explains weather graphs. In simple words, explain what relative humidity means. Use the terms \"saturated vapor pressure\", \"vapor pressure\", and \"water vapor\" in your explanation.\n",
      "\n",
      "Then, create a table of relative humidity values for different locations, using the data from the weather graphs in the weather report. The table should include the location, the relative humidity, and the temperature. The relative humidity should be in the range of 10% to 90%, and the temperature should be in the range of 0°C to 30°C. You can use the data from the weather graphs to create this table.\n",
      "\n",
      "Finally, answer the question: What is the difference between relative humidity and humidity? Use the terms \"saturated vapor pressure\", \"vapor pressure\", and \"water vapor\" in your\n"
     ]
    }
   ],
   "source": [
    "result = generator(prompt, max_new_tokens=150, do_sample=True, temperature=0.3, top_p=0.9)[0][\"generated_text\"]\n",
    "\n",
    "print(\"\\n=== Model output ===\\n\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
